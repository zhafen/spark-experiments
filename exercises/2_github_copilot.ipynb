{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac90aa55-dde6-46fe-9fad-da2c79947343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2592eff6-5767-440d-8be3-9518e338db09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b32cce0-401d-4d3c-84e0-599674912de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transactions_fp = os.path.abspath(\"../data/data_skew/transactions.parquet\")\n",
    "transactions_fp = \"file:\" + transactions_fp\n",
    "df_txn = spark.read.parquet(transactions_fp).withColumn(\"amt\", sf.col(\"amt\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21a1206-c1d7-4f38-be10-352af1b152dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_txn.display(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb5da15-398f-4d55-a157-bef2eca5fadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_fp = os.path.abspath(\"../data/data_skew/customers.parquet\")\n",
    "customers_fp = \"file:\" + customers_fp\n",
    "df_cust = spark.read.parquet(customers_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899c3630-6c25-42fd-8e9e-7216023734eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust.display(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90c4bba2-99e5-4e28-9d9a-680635220b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Optimization Exercises\n",
    "\n",
    "Complete the following exercises to improve the performance of Spark code. For each exercise, identify the performance issues and implement optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9139f97-45b4-4f97-b3a9-5aa775e3a81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 1: Inefficient Filtering\n",
    "\n",
    "**Problem:** The following code filters a large dataset multiple times. Identify and fix the performance issues.\n",
    "\n",
    "```python\n",
    "# Inefficient code - DO NOT RUN AS IS\n",
    "result = df_txn.filter(sf.col(\"amt\") > 100)\n",
    "result = result.filter(sf.col(\"status\") == \"completed\")\n",
    "result = result.filter(sf.col(\"cust_id\").isNotNull())\n",
    "result = result.select(\"transaction_id\", \"cust_id\", \"amt\", \"date\")\n",
    "result = result.filter(sf.year(\"date\") == 2024)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify the performance issues\n",
    "2. Optimize the code below\n",
    "3. Explain what optimizations you made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58a69c8f-8bba-4ced-a4a4-1cd2ec39b13f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Original code"
    }
   },
   "outputs": [],
   "source": [
    "result = df_txn.filter(sf.col(\"amt\") > 100)\n",
    "result = result.filter(sf.col(\"city\") == \"chicago\")\n",
    "result = result.filter(sf.col(\"cust_id\").isNotNull())\n",
    "result = result.select(\"txn_id\", \"cust_id\", \"amt\", \"date\")\n",
    "result = result.filter(sf.year(\"date\") == 2018)\n",
    "result.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2df8a7c-638a-48d8-8bbd-d9faa62472ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d38b2f1-81dd-4d36-9081-912b2bb2fb93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 2: Join Optimization\n",
    "\n",
    "**Problem:** The code below performs a join that could be optimized using broadcast join.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "large_df = df_txn\n",
    "small_df = df_cust.select(\"cust_id\", \"customer_segment\").distinct()\n",
    "\n",
    "result = large_df.join(small_df, \"cust_id\", \"inner\")\n",
    "result = result.groupBy(\"customer_segment\").agg(sf.sum(\"amt\").alias(\"total_amt\"))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Determine if a broadcast join is appropriate\n",
    "2. Implement the optimization\n",
    "3. Compare the query plans before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5372e82b-7144-4968-a781-17cd041a509f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aad4d6f-793e-4353-93c7-b4332933fa25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 3: Unnecessary Shuffles\n",
    "\n",
    "**Problem:** The following code causes multiple unnecessary shuffles.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "df = df_txn.groupBy(\"cust_id\").agg(sf.sum(\"amt\").alias(\"total_spent\"))\n",
    "df = df.filter(df.total_spent > 500)\n",
    "df = df.join(df_cust, \"cust_id\")\n",
    "df = df.groupBy(\"customer_segment\").agg(sf.avg(\"total_spent\").alias(\"avg_spent\"))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify where unnecessary shuffles occur\n",
    "2. Optimize the code to reduce shuffles\n",
    "3. Use `.explain()` to verify the reduction in shuffle operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb854b6-4aff-4bbe-bd29-c546f006f3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f154a0cf-abd3-4e59-a20b-f88074b99291",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 4: Caching Strategy\n",
    "\n",
    "**Problem:** The code below reuses a DataFrame multiple times but doesn't cache it effectively.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "filtered_transactions = df_txn.filter(sf.col(\"date\") >= \"2024-01-01\")\n",
    "filtered_transactions = filtered_transactions.filter(sf.col(\"amt\") > 0)\n",
    "\n",
    "# This DataFrame is used multiple times\n",
    "high_value = filtered_transactions.filter(sf.col(\"amt\") > 1000).count()\n",
    "by_status = filtered_transactions.groupBy(\"status\").count()\n",
    "avg_amt = filtered_transactions.agg(sf.avg(\"amt\")).collect()\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify where caching would be beneficial\n",
    "2. Implement appropriate caching with the right storage level\n",
    "3. Explain when to use cache() vs persist() and which storage level to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4448d1ad-bc95-44b6-b3db-09ceb5548a51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8f00134-6a0d-4282-8d6e-b7d561c7d4ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 5: Partition Optimization\n",
    "\n",
    "**Problem:** The code reads data and performs operations without considering partitioning.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "df = df_txn.repartition(200)  # Arbitrary partition count\n",
    "result = df.filter(sf.col(\"cust_id\") == \"CUST001\").groupBy(\"date\").sum(\"amt\")\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Analyze the data size and operation to determine optimal partition count\n",
    "2. Consider whether repartition or coalesce is more appropriate\n",
    "3. Optimize partitioning based on the subsequent operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d24119f-30e0-4eff-9b7a-8bd45b7dcc15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb78b749-d083-42db-a5ae-4468a09b1655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 6: UDF Optimization\n",
    "\n",
    "**Problem:** The code uses a Python UDF which is inefficient.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorize_amt(amt):\n",
    "    if amt < 100:\n",
    "        return \"low\"\n",
    "    elif amt < 500:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "result = df_txn.withColumn(\"category\", categorize_amt(sf.col(\"amt\")))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify why the UDF is inefficient\n",
    "2. Rewrite using built-in Spark functions (when, otherwise)\n",
    "3. Compare performance if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d636d47c-48fa-4be8-8f5b-63da3f07bf4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06547a06-6fce-4d0c-a18f-ef4b3ceaa50f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exercise 7: Data Skew Challenge\n",
    "\n",
    "**Problem:** The transactions dataset has data skew where a few customers have many transactions.\n",
    "\n",
    "```python\n",
    "# This code will suffer from data skew\n",
    "result = df_txn.groupBy(\"cust_id\").agg(\n",
    "    sf.count(\"*\").alias(\"transaction_count\"),\n",
    "    sf.sum(\"amt\").alias(\"total_amt\"),\n",
    "    sf.avg(\"amt\").alias(\"avg_amt\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify the skew in the data (check cust_id distribution)\n",
    "2. Implement a salting strategy or other technique to handle the skew\n",
    "3. Compare execution time before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92bd2ea2-097a-4f7e-8a0a-be28e21d989d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd7b030-3c26-4a96-b77a-ffb0d2c042c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bonus Exercise: Complex Query Optimization\n",
    "\n",
    "**Problem:** Optimize this complex query that combines multiple operations.\n",
    "\n",
    "```python\n",
    "# Complex inefficient code\n",
    "df1 = df_txn.filter(sf.col(\"amt\") > 100)\n",
    "df2 = df1.groupBy(\"cust_id\").agg(sf.sum(\"amt\").alias(\"total\"))\n",
    "df3 = df2.filter(sf.col(\"total\") > 1000)\n",
    "df4 = df3.join(df_cust, \"cust_id\")\n",
    "df5 = df4.select(\"cust_id\", \"customer_name\", \"total\", \"customer_segment\")\n",
    "df6 = df5.orderBy(\"total\", ascending=False)\n",
    "result = df6.limit(10)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "Apply all optimization techniques learned:\n",
    "- Predicate pushdown\n",
    "- Column pruning\n",
    "- Join optimization\n",
    "- Appropriate use of cache if needed\n",
    "- Efficient shuffle operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "323eb4bd-1314-4371-9e07-06e72f93c107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_github_copilot",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

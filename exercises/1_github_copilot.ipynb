{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12fa052b-65ab-42fe-92e9-866a91ab0479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac90aa55-dde6-46fe-9fad-da2c79947343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e02171b5-8b8d-4b81-8826-2fb3ee1a8a24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2592eff6-5767-440d-8be3-9518e338db09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Reading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b32cce0-401d-4d3c-84e0-599674912de5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "transactions_fp = os.path.abspath(\"../data/data_skew/transactions.parquet\")\n",
    "transactions_fp = \"file:\" + transactions_fp\n",
    "df_transactions = spark.read.parquet(transactions_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21a1206-c1d7-4f38-be10-352af1b152dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_transactions.display(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cb5da15-398f-4d55-a157-bef2eca5fadf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_fp = os.path.abspath(\"../data/data_skew/customers.parquet\")\n",
    "customers_fp = \"file:\" + customers_fp\n",
    "df_customers = spark.read.parquet(customers_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899c3630-6c25-42fd-8e9e-7216023734eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.display(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90c4bba2-99e5-4e28-9d9a-680635220b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Optimization Exercises\n",
    "\n",
    "Complete the following exercises to improve the performance of Spark code. For each exercise, identify the performance issues and implement optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e9139f97-45b4-4f97-b3a9-5aa775e3a81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Exercise 1: Inefficient Filtering\n",
    "\n",
    "**Problem:** The following code filters a large dataset multiple times. Identify and fix the performance issues.\n",
    "\n",
    "```python\n",
    "# Inefficient code - DO NOT RUN AS IS\n",
    "result = df_transactions.filter(sf.col(\"amount\") > 100)\n",
    "result = result.filter(sf.col(\"status\") == \"completed\")\n",
    "result = result.filter(sf.col(\"customer_id\").isNotNull())\n",
    "result = result.select(\"transaction_id\", \"customer_id\", \"amount\", \"date\")\n",
    "result = result.filter(sf.year(\"date\") == 2024)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify the performance issues\n",
    "2. Optimize the code below\n",
    "3. Explain what optimizations you made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Join Optimization\n",
    "\n",
    "**Problem:** The code below performs a join that could be optimized using broadcast join.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "large_df = df_transactions\n",
    "small_df = df_customers.select(\"customer_id\", \"customer_segment\").distinct()\n",
    "\n",
    "result = large_df.join(small_df, \"customer_id\", \"inner\")\n",
    "result = result.groupBy(\"customer_segment\").agg(sf.sum(\"amount\").alias(\"total_amount\"))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Determine if a broadcast join is appropriate\n",
    "2. Implement the optimization\n",
    "3. Compare the query plans before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Unnecessary Shuffles\n",
    "\n",
    "**Problem:** The following code causes multiple unnecessary shuffles.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "df = df_transactions.groupBy(\"customer_id\").agg(sf.sum(\"amount\").alias(\"total_spent\"))\n",
    "df = df.filter(df.total_spent > 500)\n",
    "df = df.join(df_customers, \"customer_id\")\n",
    "df = df.groupBy(\"customer_segment\").agg(sf.avg(\"total_spent\").alias(\"avg_spent\"))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify where unnecessary shuffles occur\n",
    "2. Optimize the code to reduce shuffles\n",
    "3. Use `.explain()` to verify the reduction in shuffle operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Caching Strategy\n",
    "\n",
    "**Problem:** The code below reuses a DataFrame multiple times but doesn't cache it effectively.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "filtered_transactions = df_transactions.filter(sf.col(\"date\") >= \"2024-01-01\")\n",
    "filtered_transactions = filtered_transactions.filter(sf.col(\"amount\") > 0)\n",
    "\n",
    "# This DataFrame is used multiple times\n",
    "high_value = filtered_transactions.filter(sf.col(\"amount\") > 1000).count()\n",
    "by_status = filtered_transactions.groupBy(\"status\").count()\n",
    "avg_amount = filtered_transactions.agg(sf.avg(\"amount\")).collect()\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify where caching would be beneficial\n",
    "2. Implement appropriate caching with the right storage level\n",
    "3. Explain when to use cache() vs persist() and which storage level to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Partition Optimization\n",
    "\n",
    "**Problem:** The code reads data and performs operations without considering partitioning.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "df = df_transactions.repartition(200)  # Arbitrary partition count\n",
    "result = df.filter(sf.col(\"customer_id\") == \"CUST001\").groupBy(\"date\").sum(\"amount\")\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Analyze the data size and operation to determine optimal partition count\n",
    "2. Consider whether repartition or coalesce is more appropriate\n",
    "3. Optimize partitioning based on the subsequent operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: UDF Optimization\n",
    "\n",
    "**Problem:** The code uses a Python UDF which is inefficient.\n",
    "\n",
    "```python\n",
    "# Inefficient code\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorize_amount(amount):\n",
    "    if amount < 100:\n",
    "        return \"low\"\n",
    "    elif amount < 500:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"high\"\n",
    "\n",
    "result = df_transactions.withColumn(\"category\", categorize_amount(sf.col(\"amount\")))\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify why the UDF is inefficient\n",
    "2. Rewrite using built-in Spark functions (when, otherwise)\n",
    "3. Compare performance if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Data Skew Challenge\n",
    "\n",
    "**Problem:** The transactions dataset has data skew where a few customers have many transactions.\n",
    "\n",
    "```python\n",
    "# This code will suffer from data skew\n",
    "result = df_transactions.groupBy(\"customer_id\").agg(\n",
    "    sf.count(\"*\").alias(\"transaction_count\"),\n",
    "    sf.sum(\"amount\").alias(\"total_amount\"),\n",
    "    sf.avg(\"amount\").alias(\"avg_amount\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "1. Identify the skew in the data (check customer_id distribution)\n",
    "2. Implement a salting strategy or other technique to handle the skew\n",
    "3. Compare execution time before and after optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Complex Query Optimization\n",
    "\n",
    "**Problem:** Optimize this complex query that combines multiple operations.\n",
    "\n",
    "```python\n",
    "# Complex inefficient code\n",
    "df1 = df_transactions.filter(sf.col(\"amount\") > 100)\n",
    "df2 = df1.groupBy(\"customer_id\").agg(sf.sum(\"amount\").alias(\"total\"))\n",
    "df3 = df2.filter(sf.col(\"total\") > 1000)\n",
    "df4 = df3.join(df_customers, \"customer_id\")\n",
    "df5 = df4.select(\"customer_id\", \"customer_name\", \"total\", \"customer_segment\")\n",
    "df6 = df5.orderBy(\"total\", ascending=False)\n",
    "result = df6.limit(10)\n",
    "```\n",
    "\n",
    "**Your Task:**\n",
    "Apply all optimization techniques learned:\n",
    "- Predicate pushdown\n",
    "- Column pruning\n",
    "- Join optimization\n",
    "- Appropriate use of cache if needed\n",
    "- Efficient shuffle operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimized code here"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_databricks_assistant",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
